<!DOCTYPE html>
<html lang="en">

<head>
    <title>Machine Translation with Recurrent Neural Networks</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.4.0/css/font-awesome.min.css" />
    <link rel="stylesheet" href="/theme/css/main.css" />
    <link rel="stylesheet" href="/theme/css/custom.css" />

    <link id="dark-theme-style" rel="stylesheet" type="text/css"         media="(prefers-color-scheme: dark)"  href="/theme/css/dark-theme.css">
    <link id="pygments-light-theme" rel="stylesheet" type="text/css"         media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"          href="/theme/css/light-theme.css">


</head>

<body >
    <div class="flex-wrapper">
        <div class="everything-but-footer">
            <div class="main-header">
                <ul>
                    <li><a href="/" class="pure-menu-heading pure-menu-link">Home</a></li> •
                    <!-- <li><a href="/pages/about.html" class="pure-menu-heading pure-menu-link">About</a></li> • -->
                    <li><a href="/pages/research.html" class="pure-menu-heading pure-menu-link">Research</a></li> •
                    <li><a href="/author/xiaoshui.html" class="pure-menu-heading pure-menu-link">Blog</a></li> •
                    <li><a href="https://github.com/xiaoshuihuang" class="pure-menu-heading pure-menu-link">GitHub</a></li>
                </ul>
            </div>

            <!-- CONTENT-->
<div class="page-container">

    <div class="article-header-container">
        <div class="background-image-container">
            <div class="background-image-small">
                <div class="title-container">
                    <h1>Machine Translation with Recurrent Neural Networks</h1>
                </div>
            </div>
        </div>
    </div>

    <div class="entry-content">
        <div class="post-meta pure-g">
<!-- <div class="pure-u-3-4 meta-data">
    <a href="/category/nlp.html" class="category">nlp</a><br />

    <a class="author" href="/author/luke.html">luke</a>
    &mdash; <abbr title="2018-05-24T10:00:00-04:00">Thursday 24 May 2018</abbr>
</div> -->

<div class="pure-u-1 meta-data">
    <a href="/category/nlp.html" class="category">nlp</a><br />
    <span title="2018-05-24T10:00:00-04:00">Thursday 24 May 2018</span>
</div>        </div>
    </div>

    <div class="entry-content">
        
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Introduction">Introduction<a class="anchor-link" href="#Introduction">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this post, we're going to build a machine learning model to translate German sentences into English. It's not quite going to be Google Translate, but it will actually come surprisingly close!</p>
<p>We'll be using the same model architecture as Google, a neural sequence-to-sequence model. For the details on Google's setup, you can read <a href="https://arxiv.org/abs/1609.08144">their paper</a> (Wu et al.). For our setup, we'll work primarily from <a href="https://arxiv.org/abs/1409.0473">this paper</a> (Bahdanau et al.), a breakthough paper that serves as the base of Google's model.</p>
<p>We will build the model from scratch with PyTorch 0.4 and Python 3. In fact, everything here is an iPython notebook (published <a href="https://drive.google.com/file/d/1zpdCOpvd_ggiw77ShLDR0NzPz15wvWpb/view?usp=sharing">here</a>) that you can run yourself. Additionally, everything is uploaded publicly to <a href="https://github.com/lukemelas/Machine-Translation">GitHub</a>.</p>
<p>At the end of the day, you'll be able to produce translations like these:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table>
<tbody>
<tr>
<td style="text-align:left"><strong>German</strong></td>
<td style="text-align:left">Dieser Tag hat unsere Sicht nachhaltig verändert .</td>
</tr>
<tr>
<td style="text-align:left"><strong>Professional Translation</strong></td>
<td style="text-align:left">And that day really changed our perspective .</td>
</tr>
<tr>
<td style="text-align:left"><strong>Our Translation</strong></td>
<td style="text-align:left">This day, our view has changed .</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align:left"><strong>German</strong></td>
<td style="text-align:left">So lernt man als Kind eine Sprache .</td>
</tr>
<tr>
<td style="text-align:left"><strong>Professional Translation</strong></td>
<td style="text-align:left">And this is what you learn when you learn a language as a child .</td>
</tr>
<tr>
<td style="text-align:left"><strong>Our Translation</strong></td>
<td style="text-align:left">This is how you learn a language as a child .</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align:left"><strong>German</strong></td>
<td style="text-align:left">Man kann eine Kultur ohne Austausch pflegen .</td>
</tr>
<tr>
<td style="text-align:left"><strong>Professional Translation</strong></td>
<td style="text-align:left">You can have culture without exchange .</td>
</tr>
<tr>
<td style="text-align:left"><strong>Our Translation</strong></td>
<td style="text-align:left">You can exchange a culture without exchange .</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This tutorial is ideally for someone with some experience with neural networks, but unfamiliar with natural language processing or machine translation. </p>
<p>For those looking to take machine translation to the next level, try out the brilliant <a href="https://github.com/OpenNMT/OpenNMT-py">OpenNMT</a> platform, also built in PyTorch.</p>
<p>Now, let's dive into translation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Overview">Overview<a class="anchor-link" href="#Overview">&#182;</a></h3><p>Our goal is to convert a German source sentence into an English sentence. To do this, we will first <em>encode</em> each word of the German sentence and then <em>decode</em> an English sentence one word at a time. During decoding, we will use <em>attention</em> to look at the encoded English words as we go along.</p>
<p>For example, when trying to translate the third sentence above, in the first step of decoding, we might <em>attend</em> to the encoding of German word "Man" in the source sentence and then produce the English word "You". For this reason, we call our model an encoder-decoder sequence-to-sequence model. This type of model is now ubiquitous in natural language processing and other areas that deal with sequences (text, speech, etc.).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="The-Data">The Data<a class="anchor-link" href="#The-Data">&#182;</a></h4><p>To train our translation model, we need a ton of German-English sentence pairs translated by professional translators. We'll use the IWSLT collection of German TED Talks translated into English. For training bigger models, researchers often use the proceedings of the European Parliament.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We preprocess the data with the <em>spacy</em> library. In preprocessing, we split our sentence into <em>tokens</em> (words) and add special <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> tokens to mark the beginning and end of sentences.  We also replace words which occur fewer than 5 times with an unknown token <code>&lt;unk&gt;</code> -- this helps us keep our vocabulary to a managable 11560 English words and 13353 German words.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you're running this code on your own computer, this preprocessing might take a little while.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">## Uncomment these lines if you have not downloaded spacy and torchtext</span>
<span class="c1"># !pip install spacy</span>
<span class="c1"># !pip install torch torchvision torchtext</span>
<span class="c1"># !python -m spacy download en</span>
<span class="c1"># !python -m spacy download de</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span><span class="o">,</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">time</span> <span class="o">,</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torchtext</span> <span class="k">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="k">import</span> <span class="n">Vectors</span><span class="p">,</span> <span class="n">GloVe</span>
<span class="n">use_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batchsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">max_sent_len</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Loads data from text files into iterators&#39;&#39;&#39;</span>

    <span class="c1"># Load text tokenizers</span>
    <span class="n">spacy_de</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;de&#39;</span><span class="p">)</span>
    <span class="n">spacy_en</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;en&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;en&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">lang</span> <span class="ow">is</span> <span class="s1">&#39;de&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">tok</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">spacy_de</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="n">lang</span> <span class="ow">is</span> <span class="s1">&#39;en&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">tok</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">spacy_en</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Invalid language&#39;</span><span class="p">)</span>

    <span class="c1"># Add beginning-of-sentence and end-of-sentence tokens </span>
    <span class="n">BOS_WORD</span> <span class="o">=</span> <span class="s1">&#39;&lt;s&gt;&#39;</span>
    <span class="n">EOS_WORD</span> <span class="o">=</span> <span class="s1">&#39;&lt;/s&gt;&#39;</span>
    <span class="n">DE</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">tokenize</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;de&#39;</span><span class="p">))</span>
    <span class="n">EN</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">tokenize</span><span class="o">=</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">init_token</span><span class="o">=</span><span class="n">BOS_WORD</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="n">EOS_WORD</span><span class="p">)</span>

    <span class="c1"># Create sentence pair dataset with max length 20</span>
    <span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">IWSLT</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span><span class="n">exts</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;.de&#39;</span><span class="p">,</span> <span class="s1">&#39;.en&#39;</span><span class="p">),</span> <span class="n">fields</span><span class="o">=</span><span class="p">(</span><span class="n">DE</span><span class="p">,</span> <span class="n">EN</span><span class="p">),</span> <span class="n">filter_pred</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="s1">&#39;src&#39;</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="s1">&#39;trg&#39;</span><span class="p">]))</span> <span class="o">&lt;=</span> <span class="n">max_sent_len</span><span class="p">)</span>

    <span class="c1"># Build vocabulary and convert text to indices</span>
    <span class="c1"># Convert words that appear fewer than 5 times to &lt;unk&gt;</span>
    <span class="k">if</span> <span class="n">vocab_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">DE</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">src</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
        <span class="n">EN</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">trg</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">DE</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">src</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">EN</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">trg</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Create iterators to process text in batches of approx. the same length</span>
    <span class="n">train_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batchsize</span><span class="p">,</span> <span class="n">device</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sort_key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">src</span><span class="p">))</span>
    <span class="n">val_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sort_key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">src</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">DE</span><span class="p">,</span> <span class="n">EN</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">val_iter</span>

<span class="c1"># Test</span>
<span class="n">timer</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">SRC</span><span class="p">,</span> <span class="n">TGT</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">val_iter</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;This is a test of our preprocessing function. It took </span><span class="si">{:.1f}</span><span class="s1"> seconds to load the data. </span>
<span class="s1">Our German vocab has size </span><span class="si">{}</span><span class="s1"> and our English vocab has size </span><span class="si">{}</span><span class="s1">.</span>
<span class="s1">Our training data has </span><span class="si">{}</span><span class="s1"> batches, each with </span><span class="si">{}</span><span class="s1"> sentences, and our validation data has </span><span class="si">{}</span><span class="s1"> batches.&#39;&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
<span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">timer</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">SRC</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">TGT</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">),</span> <span class="n">train_iter</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_iter</span><span class="p">)))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>This is a test of our preprocessing function. It took 202.0 seconds to load the data. 
Our German vocab has size 13353 and our English vocab has size 11560.
Our training data has 7443 batches, each with 16 sentences, and our validation data has 570 batches.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Model">The Model<a class="anchor-link" href="#The-Model">&#182;</a></h3><p><img src="https://github.com/lukemelas/lukemelas.github.io-src/raw/master/content/assets/images/translation/translation_diagram.png" style="max-width: 100%" alt="Attention"></p>
<h4 id="Word-Embeddings">Word Embeddings<a class="anchor-link" href="#Word-Embeddings">&#182;</a></h4><p>The first step of both the encoder and decoder is to convert the input words to into vectors, a form our model can work with. We do so with word embeddings, which are mappings from each word in our vocab to a vector in some high-dimensional space (say, 300 dimensions).</p>
<p>Word embeddings are a subject for an entirely separate blog post, but the basic idea is that word vectors capture some semantic meaning: the vector for <code>dog</code> is closer to the vector for <code>cat</code> than the vector for <code>asparagus</code>.</p>
<p>You can get your own vectors from a source like<code>GloVe</code> or <code>fastText</code>, or you can use the ones I've generated and uploaded to GitHub (links below).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>wget https://github.com/lukemelas/Machine-Translation/raw/master/scripts/emb-13353-de.npy https://github.com/lukemelas/Machine-Translation/raw/master/scripts/emb-11560-en.npy
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">load_embeddings</span><span class="p">(</span><span class="n">SRC</span><span class="p">,</span> <span class="n">TGT</span><span class="p">,</span> <span class="n">np_src_file</span><span class="p">,</span> <span class="n">tgt_file</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Load English and German embeddings from saved numpy files&#39;&#39;&#39;</span>
    <span class="n">emb_tr_src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">np_src_file</span><span class="p">))</span>
    <span class="n">emb_tr_tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">np_tgt_file</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">emb_tr_src</span><span class="p">,</span> <span class="n">emb_tr_tgt</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Encoder">Encoder<a class="anchor-link" href="#Encoder">&#182;</a></h4><p>Our encoder (red in the model diagram above) is a bidirectional recurrent neural network. Bidirectional simply means that we run the model both backwards and forwards along the sentence.</p>
<p>Our encoder outputs a vector for each word in the source sentence. All these vectors together are called a <em>memory bank</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">EncoderLSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="n">bidirectional</span> 

        <span class="c1"># Create embedding and LSTM</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Embed text, get initial LSTM hidden state, and encode with LSTM&#39;&#39;&#39;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># embedding</span>
        <span class="n">h0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># initial state of LSTM</span>
        <span class="n">memory_bank</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span> <span class="c1"># encoding</span>
        <span class="k">return</span> <span class="n">memory_bank</span><span class="p">,</span> <span class="n">h</span>

    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Create initial hidden state of zeros: 2-tuple of num_layers x batch size x hidden dim&#39;&#39;&#39;</span>
        <span class="n">num_layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="mi">2</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>
        <span class="n">init</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">)</span>
        <span class="n">init</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_gpu</span> <span class="k">else</span> <span class="n">init</span>
        <span class="n">h0</span> <span class="o">=</span> <span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">init</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Attention">Attention<a class="anchor-link" href="#Attention">&#182;</a></h4><p>Attention enables our decoder to look at our encoded source words while translating. We use dot-product attention, which means we take the dot product of our intermediate decoder output and our encoder output. We then take a weighted sum of our encoder vectors, using this dot product as the weight.</p>
<p>There are lots of other types of attention, described in more detail <a href="http://ruder.io/deep-learning-nlp-best-practices/index.html#attention">here</a>, but we use dot-product attention because it is simple and works well.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">h_dim</span><span class="o">=</span><span class="mi">300</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">pad_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_e</span><span class="p">,</span> <span class="n">out_e</span><span class="p">,</span> <span class="n">out_d</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Produces context with attention distribution&#39;&#39;&#39;</span>

        <span class="c1"># Deal with bidirectional encoder, move batches first</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span><span class="p">:</span> <span class="c1"># sum hidden states for both directions</span>
            <span class="n">out_e</span> <span class="o">=</span> <span class="n">out_e</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_e</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">out_e</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_e</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">out_e</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            
        <span class="c1"># Move batches first</span>
        <span class="n">out_e</span> <span class="o">=</span> <span class="n">out_e</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># b x sl x hd</span>
        <span class="n">out_d</span> <span class="o">=</span> <span class="n">out_d</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># b x tl x hd</span>

        <span class="c1"># Dot product attention, softmax, and reshape</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">out_e</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">out_d</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># (b x sl x hd) (b x hd x tl) --&gt; (b x sl x tl)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># --&gt; b x tl x sl</span>

        <span class="c1"># Get attention distribution</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">out_e</span><span class="p">)</span> <span class="c1"># --&gt; b x tl x hd</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># --&gt; tl x b x hd</span>
        <span class="k">return</span> <span class="n">context</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Decoder">Decoder<a class="anchor-link" href="#Decoder">&#182;</a></h4><p>Our decoder (blue and yellow in the diagram) is a recurrent neural network. Within our decoder, we have an <em>attention</em> layer, which looks at the memory bank from the encoder.</p>
<p>We begin by feeding in the start token <code>&lt;s&gt;</code>. Our decoder tries to predict the next word by outputting a distribution over all words in the vocabulary. During training, we know the ground truth sentence, so we feed it into the decoder word-by-word at each step. We penalize the model's predictions using a cross-entropy loss function. During testing, we do not know the ground truth, so we use a prediction of the model as input to the next time step. We'll discuss this process in more detail below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DecoderLSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_p</span> <span class="o">=</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">dropout_p</span>
        
        <span class="c1"># Create embedding and LSTM</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h0</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Embed text and pass through LSTM&#39;&#39;&#39;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">h</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Beam-Search">Beam Search<a class="anchor-link" href="#Beam-Search">&#182;</a></h4><p>At test time, we need to use the output of our decoder as the input to the model at the next time step. We could do this by taking the most likely word each time, a strategy known as greedy search. Here, we'll use a fancier method known as beam search, which keeps around a list of likely partial sentences during decoding.</p>
<p>Beam search is a bit complicated, but I've decided to include it because few other tutorials do, and because it really does improve translation quality. To make it simpler, I've only implemented it for batch size = 1. Since beam search will be a method of our final model class, the code is in the section below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Final-model">Final model<a class="anchor-link" href="#Final-model">&#182;</a></h4><p>Our final model combines the encoder, attention, decoder, and beam search. We call it <em>Seq2seq</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Seq2seq</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_src</span><span class="p">,</span> <span class="n">embedding_tgt</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="n">bi</span><span class="p">,</span> <span class="n">tokens_bos_eos_pad_unk</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Seq2seq</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Store hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span> <span class="o">=</span> <span class="n">h_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size_tgt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim_tgt</span> <span class="o">=</span> <span class="n">embedding_tgt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bos_token</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span> <span class="o">=</span> <span class="n">tokens_bos_eos_pad_unk</span>

        <span class="c1"># Create encoder, decoder, attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderLSTM</span><span class="p">(</span><span class="n">embedding_src</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="n">bi</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">DecoderLSTM</span><span class="p">(</span><span class="n">embedding_tgt</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">*</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bi</span> <span class="k">else</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">pad_token</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="n">bi</span><span class="p">,</span> <span class="n">h_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">)</span>

        <span class="c1"># Create linear layers to combine context and hidden state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim_tgt</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_dim_tgt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size_tgt</span><span class="p">)</span>
        
        <span class="c1"># Share weights between decoder embedding and output </span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span> <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        
        <span class="c1"># Encode</span>
        <span class="n">out_e</span><span class="p">,</span> <span class="n">final_e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        
        <span class="c1"># Decode</span>
        <span class="n">out_d</span><span class="p">,</span> <span class="n">final_d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">final_e</span><span class="p">)</span>
        
        <span class="c1"># Attend</span>
        <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">out_e</span><span class="p">,</span> <span class="n">out_d</span><span class="p">)</span>
        <span class="n">out_cat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">out_d</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> 
        
        <span class="c1"># Predict (returns probabilities)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">out_cat</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">beam_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span> 
        <span class="sd">&#39;&#39;&#39;Predict top 1 sentence using beam search. Note that beam_size=1 is greedy search.&#39;&#39;&#39;</span>
        <span class="n">beam_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beam_search</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">beam_size</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span> <span class="c1"># returns top beam_size options (as list of tuples)</span>
        <span class="n">top1</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># a list of word indices (as ints)</span>
        <span class="k">return</span> <span class="n">top1</span>

    <span class="k">def</span> <span class="nf">beam_search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">beam_size</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">remove_tokens</span><span class="o">=</span><span class="p">[]):</span>
        <span class="sd">&#39;&#39;&#39;Returns top beam_size sentences using beam search. Works only when src has batch size 1.&#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span> <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        
        <span class="c1"># Encode</span>
        <span class="n">outputs_e</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="c1"># batch size = 1</span>
        
        <span class="c1"># Start with &#39;&lt;s&gt;&#39;</span>
        <span class="n">init_lprob</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e10</span>
        <span class="n">init_sent</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">bos_token</span><span class="p">]</span>
        <span class="n">best_options</span> <span class="o">=</span> <span class="p">[(</span><span class="n">init_lprob</span><span class="p">,</span> <span class="n">init_sent</span><span class="p">,</span> <span class="n">states</span><span class="p">)]</span> <span class="c1"># beam</span>
        
        <span class="c1"># Beam search</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">beam_size</span> <span class="c1"># store best k options</span>
        <span class="k">for</span> <span class="n">length</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span> <span class="c1"># maximum target length</span>
            <span class="n">options</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># candidates </span>
            <span class="k">for</span> <span class="n">lprob</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">current_state</span> <span class="ow">in</span> <span class="n">best_options</span><span class="p">:</span>
                <span class="c1"># Prepare last word</span>
                <span class="n">last_word</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">last_word</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token</span><span class="p">:</span>
                    <span class="n">last_word_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">last_word</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span> <span class="n">last_word_input</span> <span class="o">=</span> <span class="n">last_word_input</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
                    <span class="c1"># Decode</span>
                    <span class="n">outputs_d</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">last_word_input</span><span class="p">,</span> <span class="n">current_state</span><span class="p">)</span>
                    <span class="c1"># Attend</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">outputs_e</span><span class="p">,</span> <span class="n">outputs_d</span><span class="p">)</span>
                    <span class="n">out_cat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">outputs_d</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">out_cat</span><span class="p">)</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                    <span class="c1"># Block predictions of tokens in remove_tokens</span>
                    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">remove_tokens</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10e10</span>
                    <span class="n">lprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="c1"># log softmax</span>
                    <span class="c1"># Add top k candidates to options list for next word</span>
                    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">lprobs</span><span class="p">,</span> <span class="n">k</span><span class="p">)[</span><span class="mi">1</span><span class="p">]:</span> 
                        <span class="n">option</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">lprobs</span><span class="p">[</span><span class="n">index</span><span class="p">])</span> <span class="o">+</span> <span class="n">lprob</span><span class="p">,</span> <span class="n">sentence</span> <span class="o">+</span> <span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">new_state</span><span class="p">)</span> 
                        <span class="n">options</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">option</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span> <span class="c1"># keep sentences ending in &#39;&lt;/s&gt;&#39; as candidates</span>
                    <span class="n">options</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">lprob</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">current_state</span><span class="p">))</span>
            <span class="n">options</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># sort by lprob</span>
            <span class="n">best_options</span> <span class="o">=</span> <span class="n">options</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="c1"># place top candidates in beam</span>
        <span class="n">best_options</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">best_options</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training">Training<a class="anchor-link" href="#Training">&#182;</a></h3><p>The section above was a bit complicated, but we're almost done. We just have to set up our training and validation loops.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">AverageMeter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&#39;&#39;&#39;A handy class for moving averages&#39;&#39;&#39;</span> 
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="n">val</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">n</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">n</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">avg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At each iteration of training, we update our model weights with gradient descent.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">val_iter</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">):</span>  
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
      
        <span class="c1"># Validate model</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
          <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">val_iter</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span> 
          <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Validating Epoch [</span><span class="si">{e}</span><span class="s1">/</span><span class="si">{num_e}</span><span class="s1">]</span><span class="se">\t</span><span class="s1"> Average loss: </span><span class="si">{l:.3f}</span><span class="se">\t</span><span class="s1"> Perplexity: </span><span class="si">{p:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">e</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">num_e</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">val_loss</span><span class="p">])</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>

        <span class="c1"># Train model</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span> 
            <span class="n">src</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">src</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_gpu</span> <span class="k">else</span> <span class="n">batch</span><span class="o">.</span><span class="n">src</span>
            <span class="n">tgt</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">trg</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_gpu</span> <span class="k">else</span> <span class="n">batch</span><span class="o">.</span><span class="n">trg</span>
            
            <span class="c1"># Forward, backprop, optimizer</span>
            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span>

            <span class="c1"># Remove &lt;s&gt; from target and &lt;/s&gt; from scores (output)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">tgt</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>           

            <span class="c1"># Reshape for loss function</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
            <span class="n">tgt</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

            <span class="c1"># Pass through loss function</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span> 
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="c1"># Clip gradient norms and step optimizer</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Log within epoch</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;Epoch [</span><span class="si">{e}</span><span class="s1">/</span><span class="si">{num_e}</span><span class="s1">]</span><span class="se">\t</span><span class="s1"> Batch [</span><span class="si">{b}</span><span class="s1">/</span><span class="si">{num_b}</span><span class="s1">]</span><span class="se">\t</span><span class="s1"> Loss: </span><span class="si">{l:.3f}</span><span class="s1">&#39;&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="o">=</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_e</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">num_b</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">),</span> <span class="n">l</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">))</span>

        <span class="c1"># Log after each epoch</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;Epoch [</span><span class="si">{e}</span><span class="s1">/</span><span class="si">{num_e}</span><span class="s1">] complete. Loss: </span><span class="si">{l:.3f}</span><span class="s1">&#39;&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="o">=</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_e</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>During training, we simply calculate the log likelihood of the ground truth sentence under our model. The exponential of this value is known as <em>perplexity</em>. If you are interested, there are more details on <a href="https://en.wikipedia.org/wiki/Perplexity">Wikipedia</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">val_iter</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Calculate losses by teacher forcing on the validation set&#39;&#39;&#39;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_iter</span><span class="p">):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">src</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_gpu</span> <span class="k">else</span> <span class="n">batch</span><span class="o">.</span><span class="n">src</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">trg</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_gpu</span> <span class="k">else</span> <span class="n">batch</span><span class="o">.</span><span class="n">trg</span>
        
        <span class="c1"># Forward </span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>           
        
        <span class="c1"># Reshape for loss function</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">num_words</span> <span class="o">=</span> <span class="p">(</span><span class="n">tgt</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        
        <span class="c1"># Calculate loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span> 
        <span class="n">losses</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
    <span class="k">return</span> <span class="n">losses</span><span class="o">.</span><span class="n">avg</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we need an actual predict function, to see our translations!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predict_from_text</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">,</span> <span class="n">SRC</span><span class="p">,</span> <span class="n">TGT</span><span class="p">):</span>
    <span class="n">sent_german</span> <span class="o">=</span> <span class="n">input_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span> <span class="c1"># sentence --&gt; list of words</span>
    <span class="n">sent_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">SRC</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">SRC</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span> <span class="k">else</span> <span class="n">SRC</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sent_german</span><span class="p">]</span>
    <span class="n">sent</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">sent_indices</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span> <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># reshape to sl x bs</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;German: &#39;</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">SRC</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sent_indices</span><span class="p">]))</span>
    <span class="c1"># Predict five sentences with beam search </span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">beam_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># returns list of 5 lists of word indices</span>
    <span class="n">out</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">TGT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">pred</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;English: &#39;</span> <span class="o">+</span> <span class="n">out</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Translation">Translation<a class="anchor-link" href="#Translation">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's time to run our model. First we load our embeddings.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load embeddings</span>
<span class="n">embedding_src</span><span class="p">,</span> <span class="n">embedding_tgt</span> <span class="o">=</span> <span class="n">load_embeddings</span><span class="p">(</span><span class="n">SRC</span><span class="p">,</span> <span class="n">TGT</span><span class="p">,</span> <span class="s1">&#39;emb-13353-de.npy&#39;</span><span class="p">,</span> <span class="s1">&#39;emb-11560-en.npy&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we create our model and move it onto the GPU.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create model </span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">TGT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;&lt;s&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;/s&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">]]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Seq2seq</span><span class="p">(</span><span class="n">embedding_src</span><span class="p">,</span> <span class="n">embedding_tgt</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">tokens_bos_eos_pad_unk</span><span class="o">=</span><span class="n">tokens</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_gpu</span> <span class="k">else</span> <span class="n">model</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we make our cross entropy loss function (criterion) and optimizer. For our optimizer, we'll use Adam, but you can definitely use other loss functions (SGD, RMSProp, Adamax, etc.) as well.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create weight to mask padding tokens for loss function</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">TGT</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>
<span class="n">weight</span><span class="p">[</span><span class="n">TGT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_gpu</span> <span class="k">else</span> <span class="n">weight</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create loss function and optimizer</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-3</span><span class="p">)</span> 
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we can train our model!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[61]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="p">(</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">val_iter</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Validating Epoch [0/50]	 Average loss: 3.439	 Perplexity: 31.152
Epoch [1/50]	 Batch [10/7443]	 Loss: 3.980
Epoch [1/50]	 Batch [1010/7443]	 Loss: 3.609
Epoch [1/50]	 Batch [2010/7443]	 Loss: 3.446
Epoch [1/50]	 Batch [3010/7443]	 Loss: 3.336
Epoch [1/50]	 Batch [4010/7443]	 Loss: 3.252
Epoch [1/50]	 Batch [5010/7443]	 Loss: 3.182
Epoch [1/50]	 Batch [6010/7443]	 Loss: 3.122
Epoch [1/50]	 Batch [7010/7443]	 Loss: 3.073
Epoch [1/50] complete. Loss: 3.052
Validating Epoch [1/50]	 Average loss: 2.274	 Perplexity: 9.716
...
</pre>
</div>
</div>

</div>

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Training takes quite a while, so I've included a pretrained model at the link below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[62]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>wget https://www.dropbox.com/s/qu18vt3jisplchd/model.pkl
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;model.pkl&#39;</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_gpu</span> <span class="k">else</span> <span class="n">model</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can validate that the pretrained model words:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[67]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">val_iter</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span> 
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average loss: </span><span class="si">{l:.3f}</span><span class="se">\t</span><span class="s1"> Perplexity: </span><span class="si">{p:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">l</span><span class="o">=</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">val_loss</span><span class="p">])</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Average loss: 1.865	 Perplexity: 6.459
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we can translate some German sentences! Let's try a few from the German newspaper <em>Süddeutsche Zeitung</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[103]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="s2">&quot;Ich kenne nur Berge, ich bleibe in den Bergen und ich liebe die Berge .&quot;</span>
<span class="n">predict_from_text</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">SRC</span><span class="p">,</span> <span class="n">TGT</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>German: Ich kenne nur &lt;unk&gt; ich bleibe in den Bergen und ich liebe die Berge .
English: I only know I &#39;m staying in the hills , and I love the mountains .
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[104]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="s2">&quot;Ihre Bergung erwies sich als komplizierter als gedacht .&quot;</span> 
<span class="n">predict_from_text</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">SRC</span><span class="p">,</span> <span class="n">TGT</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>German: Ihre &lt;unk&gt; erwies sich als komplizierter als gedacht .
English: Her &lt;unk&gt; turned out to be more complicated than that .
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion">&#182;</a></h3><p>In this post, we built a neural machine translation system in PyTorch. With just a few more additions, a library full of training data, and a warehouse full of computing power, you'll have your very own Google Translate!</p>
<p>If you found these results interesting, there are lots of exciting extensions:</p>
<ul>
<li>Unsupervise machine translation -- translating without paired training data: <a href="https://arxiv.org/abs/1804.07755">Lample et al.</a></li>
<li>Multilingual machine translation -- translating between lots of languages: <a href="https://arxiv.org/abs/1611.04558">Johnson et al.</a></li>
<li>The details of Google's system: <a href="https://arxiv.org/abs/1609.08144">Wu et al.</a></li>
</ul>
<p>I hope you enjoyed the dive into machine translation. If you would like to see anything else on the blog in the future, reach out to me!</p>

</div>
</div>
</div>
    </div>
  </div>

    </div>

    <!-- <footer>
        <div class="tags">
            <a href="/tag/rnns.html">rnns</a>
            <a href="/tag/rnn.html">rnn</a>
            <a href="/tag/machine-translation.html">machine translation</a>
            <a href="/tag/translate.html">translate</a>
            <a href="/tag/google-translate.html">google translate</a>
            <a href="/tag/google.html">google</a>
            <a href="/tag/machine-learning.html">machine learning</a>
            <a href="/tag/neural-networks.html">neural networks</a>
            <a href="/tag/ai.html">ai</a>
        </div>
        <div class="pure-g post-footer">
            <div class="pure-u-1 pure-u-md-1-2">
                <div class="pure-g poster-info">
                    <div class="pure-u-3-4">
                        <h3 class="author-name"><a href="/author/luke.html">luke</a></h3>
                        <p class="author-description">
                        </p>
                    </div>
                </div>
            </div>


        </div>
    </footer>

 -->

</div>

            <!-- END CONTENT-->

        </div>
        <footer class="index-footer">
            <a href="/" title="Luke Melas-Kyriazi">Luke Melas-Kyriazi</a>

            <!-- THEME COLOR -->
            <a class="footer-dark-tag" href="javascript:void(0)" onclick="window.theme.switch(`dark`)">Dark</a>
            <a class="footer-light-tag" href="javascript:void(0)" onclick="window.theme.switch(`light`)">Light</a>

            <!-- data-enable-auto-detect-theme="True" -->
            <script id="dark-theme-script" src="/theme/dark-theme/dark-theme.js"
                data-enable-auto-detect-theme="false" data-default-theme="light"
                type="text/javascript">
            </script>
            </p>

        </footer>
    </div>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-119150275-1', 'auto');
      ga('send', 'pageview');

    </script>
</body>

</html>