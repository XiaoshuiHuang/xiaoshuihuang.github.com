<!doctype html>
<html>
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Research of Xiaoshui Huang</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Xiaoshui Huang</h1>
        <p>Postdoctoral Research Associate<br><a href="https://sydney.edu.au/medicine-health/about/our-people/academic-staff/xiaoshui-huang.html">The University of Sydney</a></p>
        <p>Visiting Fellow<br><a href="https://www.uts.edu.au/staff/xiaoshui.huang">University of Technology Sydney</a></p>
		<h3><p class="view"><a href="https://xiaoshuihuang.github.io/">Home</a></p></h3>
        <h3><p class="view"><a href="https://xiaoshuihuang.github.io/research.html">Research</a></p></h3>
		<h3><p class="view"><a href="https://xiaoshuihuang.github.io/news.html">News & Activities</a></p></h3>
		<h3><p class="view"><a href="https://xiaoshuihuang.github.io/research/CV_Xiaoshui_Huang.pdf">CV</a></p></h3>  

    <p class="view"><b>Social</b><br>
        <a href="mailto:xiaoshui.huang@sydney.edu.au" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
        <a href="https://scholar.google.ca/citations?user=rp7mYNsAAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
        <a href="https://github.com/XiaoshuiHuang" target="_blank"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
        <a href="https://www.linkedin.com/in/xiaoshuihuang/" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a><br>

    <p><b>Contact:</b><br>ACRF Image X Institute<br>Faculty of Medicine and Health<br>The University of Sydney<br><br>Room 220, Level 2, 1 Central Ave, Eveleigh<br>NSW, Australia 2015<br><br>Phone:+61 2 8627 1110</p>
      </header>
	  
      
	<section>
    <h1><a id="published-papers-" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h1>
    </section>
	
	<section>
	<h2> 2019</h2>
	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		Fast Registration for cross-source point clouds by using weak regional aﬃnity and pixel-wise reﬁnement.
		</li></b>
		<!-- add authors below -->
		Xiaoshui Huang, Lixin Fan, Qiang Wu, Jian Zhang, Chun Yuan
		<br><i> 
		<!-- add publication name below -->
		International Conference of Multimedia Expro (ICME)
		</i>,
		<!-- add published year below -->
		2019 
		.<br> 
		<!-- paper related links -->
		<a href="https://arxiv.org/abs/1903.04630" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.com/blob/master/research/2019-tensor.PNG?raw=true" height="150" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		Many types of 3D acquisition sensors have emerged in recent years and point cloud has been widely used in many areas. Accurate and fast registration of cross-source 3D point clouds from different sensors is an emerged research problem in computer vision. This problem is extremely challenging because cross-source point clouds contain a mixture of various variances, such as density, partial overlap, large noise and outliers, viewpoint changing. In this paper, an algorithm is proposed to align cross-source point clouds with both high accuracy and high efficiency. There are two main contributions: firstly, two components, the weak region affinity and pixel-wise refinement, are proposed to maintain the global and local information of 3D point clouds. Then, these two components are integrated into an iterative tensor-based registration algorithm to solve the cross-source point cloud registration problem. We conduct experiments on synthetic cross-source benchmark dataset and real cross-source datasets. Comparison with six state-of-the-art methods, the proposed method obtains both higher efficiency and accuracy.
		</div></p>
	    <!-- ================================================================================================================================== -->

	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		 KPSNET: Keypoint detection and feature extraction for point cloud registration.
		</li></b>
		<!-- add authors below -->
		Anan Du, Xiaoshui Huang, Jian Zhang, Lingxiang Yao, Qiang Wu.
		<br><i> 
		<!-- add publication name below -->
		International Conference on Image Processing (ICIP)
		</i>,
		<!-- add published year below -->
		2019 
		.<br> 
		<!-- paper related links -->
		<a href="https://ieeexplore.ieee.org/abstract/document/8803365" target="_blank">PDF</a> &nbsp;&nbsp;&nbsp;&nbsp;
		<a href="" >Code (coming soon)</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.com/blob/master/research/2019-keypoint.PNG?raw=true" height="150" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		This paper presents the KPSNet, a KeyPoint Siamese Network to simultaneously learn task-desirable keypoint detector and feature extractor. The keypoint detector is optimized to predict a score vector, which signifies the probability of each candidate being a keypoint. The feature extractor is optimized to learn robust features of keypoints by exploiting the correspondence between the keypoints generated from two inputs, respectively. For training, the KPSNet does not require to manually annotate keypoints and local patches pairwise. Instead, we design an alignment module to establish the correspondence between the two inputs and generate positive and negative samples on-the-fly. Therefore, our method can be easily extended to new scenes. We test the proposed method on the open-source benchmark and experiments show the validity of our method.
		</div></p>
	    <!-- ================================================================================================================================== -->
	</section>
	
	<section>
	<h2> 2018</h2>
	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		Attention-based Transactional Context Embedding for Next-Item Recommendation.
		</li></b>
		<!-- add authors below -->
		Shoujin Wang, Liang Hu, Longbing Cao, Xiaoshui Huang, Defu Lian, Wei Liu
		<br><i> 
		<!-- add publication name below -->
		The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)
		</i>,
		<!-- add published year below -->
		2018 
		.<br> 
		<!-- paper related links -->
		<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16318" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.com/blob/master/research/2018-recommendation.PNG?raw=true" height="300" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		To recommend the next item to a user in a transactional context is practical yet challenging in applications such as marketing campaigns. Transactional context refers to the items that are observable in a transaction. Most existing transactionbased recommender systems (TBRSs) make recommendations by mainly considering recently occurring items instead of all the ones observed in the current context. Moreover, they often assume a rigid order between items within a transaction, which is not always practical. More importantly, a long transaction often contains many items irreverent to the next choice, which tends to overwhelm the influence of a few truely relevant ones. Therefore, we posit that a good TBRS should not only consider all the observed items in the current transaction but also weight them with different relevance to build an attentive context that outputs the proper next item with a high probability. To this end, we design an effective attentionbased transaction embedding model (ATEM) for context embedding to weight each observed item in a transaction without assuming order. The empirical study on real-world transaction datasets proves that ATEM significantly outperforms the state-of-the-art methods in terms of both accuracy and novelty.
		</div></p>
	    <!-- ================================================================================================================================== -->
	</section>
	
	<section>
	<h2> 2017</h2>
	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		A coarse-to-ﬁne algorithm for matching and registration in 3D cross-sourced point clouds.
		</li></b>
		<!-- add authors below -->
		Xiaoshui Huang, Lixin Fan, Qiang Wu, Jian Zhang, Chun Yuan.
		<br><i> 
		<!-- add publication name below -->
		Transactions on Circuits and Systems for Video Technology (T-CSVT)
		</i>,
		<!-- add published year below -->
		2017 
		.<br> 
		<!-- paper related links -->
		<a href="https://ieeexplore.ieee.org/document/7987811" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.com/blob/master/research/2017-matching.PNG?raw=true" height="300" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		We propose an efficient method to deal with the matching and registration problem found in cross-source point clouds captured by different types of sensors. This task is especially challenging due to the presence of density variation, scale difference, a large proportion of noise and outliers, missing data, and viewpoint variation. The proposed method has two stages: in the coarse matching stage, we use the ensemble of shape functions descriptor to select potential K regions from the candidate point clouds for the target. In the fine stage, we propose a scale embedded generative Gaussian mixture models registration method to refine the results from the coarse matching stage. Following the fine stage, both the best region and accurate camera pose relationships between the candidates and target are found. We conduct experiments in which we apply the method to two applications: one is 3D object detection and localization in street-view outdoor (LiDAR/VSFM) cross-source point clouds and the other is 3D scene matching and registration in indoor (KinectFusion/VSFM) cross-source point clouds. The experiment results show that the proposed method performs well when compared with the existing methods. It also shows that the proposed method is robust under various sensing techniques, such as LiDAR, Kinect, and RGB camera.
		</div></p>
	    <!-- ================================================================================================================================== -->
		
		<!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		A Systematic Approach for Cross-source Point Cloud Registration by Preserving Macro and Micro Structures.
		</li></b>
		<!-- add authors below -->
		Xiaoshui Huang, Jian Zhang, Lixin Fan, Qiang Wu, Chun Yuan.
		<br><i> 
		<!-- add publication name below -->
		Transactions on Image Processing (T-IP)
		</i>,
		<!-- add published year below -->
		2017 
		.<br> 
		<!-- paper related links -->
		<a href="https://arxiv.org/abs/1608.05143" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.com/blob/master/research/2017-macro.PNG?raw=true" height="200" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		We propose a systematic approach for registering cross-source point clouds. The compelling need for cross-source point cloud registration is motivated by the rapid development of a variety of 3D sensing techniques, but many existing registration methods face critical challenges as a result of the large variations in cross-source point clouds. This paper therefore illustrates a novel registration method which successfully aligns two cross-source point clouds in the presence of significant missing data, large variations in point density, scale difference and so on. The robustness of the method is attributed to the extraction of macro and micro structures. Our work has three main contributions: (1) a systematic pipeline to deal with cross-source point cloud registration; (2) a graph construction method to maintain macro and micro structures; (3) a new graph matching method is proposed which considers the global geometric constraint to robustly register these variable graphs. Compared to most of the related methods, the experiments show that the proposed method successfully registers in cross-source datasets, while other methods have difficulty achieving satisfactory results. The proposed method also shows great ability in same-source datasets.
		</div></p>
	    <!-- ================================================================================================================================== -->
		
	</section>
	
	<section>
	<h2> 2016</h2>
	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		A coarse-to-ﬁne algorithm for registration in 3D street-view cross-source point clouds.
		</li></b>
		<!-- add authors below -->
		Xiaoshui Huang, Jian Zhang, Qiang Wu, Lixin Fan, Chun Yuan.
		<br><i> 
		<!-- add publication name below -->
		International Conference on Digital Image Computing: Techniques and Applications (DICTA)
		</i>,
		<!-- add published year below -->
		2016
		.<br> 
		<!-- paper related links -->
		<a href="https://arxiv.org/pdf/1610.07324.pdf" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.com/blob/master/research/2016-registration.PNG?raw=true" height="250" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		With the development of numerous 3D sensing
		technologies, object registration on cross-source point cloud
		has aroused researchers’ interests. When the point clouds are
		captured from different kinds of sensors, there are large and
		different kinds of variations. In this study, we address an even
		more challenging case in which the differently-source point clouds
		are acquired from a real street view. One is produced directly
		by the LiDAR system and the other is generated by using VSFM
		software on image sequence captured from RGB cameras. When
		it confronts to large scale point clouds, previous methods mostly
		focus on point-to-point level registration, and the methods have
		many limitations.The reason is that the least mean error strategy
		shows poor ability in registering large variable cross-source point
		clouds. In this paper, different from previous ICP-based methods,
		and from a statistic view, we propose a effective coarse-to-fine
		algorithm to detect and register a small scale SFM point cloud
		in a large scale Lidar point cloud. Seen from the experimental
		results, the model can successfully run on LiDAR and SFM point
		clouds, hence it can make a contribution to many applications,
		such as robotics and smart city development
		</div></p>
	    <!-- ================================================================================================================================== -->
		
		<!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		Real Time Complete Dense Depth Reconstruction for a Monocular Camera.
		</li></b>
		<!-- add authors below -->
		Xiaoshui Huang, Lixin Fan, Jian Zhang, Qiang Wu, and Chun Yuan.
		<br><i> 
		<!-- add publication name below -->
		 IEEE Conference on Computer Vision and Pattern Recognition Workshops
		</i>,
		<!-- add published year below -->
		2016 
		.<br> 
		<!-- paper related links -->
		<a href="http://openaccess.thecvf.com/content_cvpr_2016_workshops/w17/papers/Huang_Real_Time_Complete_CVPR_2016_paper.pdf" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.com/blob/master/research/2016-depth.PNG?raw=true" height="150" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		In this paper, we aim to solve the problem of estimating complete dense depth maps from a monocular moving camera. By ’complete’, we mean depth information is estimated for every pixel and detailed reconstruction is achieved. Although this problem has previously been attempted, the accuracy of complete dense depth reconstruction is a remaining problem. We propose a novel system which produces accurate complete dense depth map. The new system consists of two subsystems running in separated threads, namely, dense mapping and sparse patch-based tracking. For dense mapping, a new projection error computation method is proposed to enhance the gradient component in estimated depth maps. For tracking, a new sparse patch-based tracking method estimates camera pose by minimizing a normalized error term. The experiments demonstrate that the proposed method obtains improved performance in terms of completeness and accuracy compared to three state-ofthe-art dense reconstruction methods VSFM+CMVC, LSDSLAM and REMODE.
    	</div></p>
	    <!-- ================================================================================================================================== -->
		
	</section>
	
	<section>
	<h2> 2015</h2>
	    <!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		Graph Cuts Stereo Matching Based on Patch-Match and Ground Control Points Constraint.
		</li></b>
		<!-- add authors below -->
		Xiaoshui Huang, Chun Yuan, and Jian Zhang.
		<br><i> 
		<!-- add publication name below -->
		Paciﬁc Rim Conference on Multimedia (PCM)
		</i>,
		<!-- add published year below -->
		2015
		.<br> 
		<!-- paper related links -->
		<a href="https://link.springer.com/chapter/10.1007/978-3-319-24078-7_2" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.com/blob/master/research/2015-depth.PNG?raw=true" height="200" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		Stereo matching methods based on Patch-Match obtain good results on complex texture regions but show poor ability on low texture regions. In this paper, a new method that integrates Patch-Match and graph cuts (GC) is proposed in order to achieve good results in both complex and low texture regions. A label is randomly assigned for each pixel and the label is optimized through propagation process. All these labels constitute a label space for each iteration in GC. Also, a Ground Control Points (GCPs) constraint term is added to the GC to overcome the disadvantages of Patch-Match stereo in low texture regions. The proposed method has the advantage of the spatial propagation of Patch-Match and the global property of GC. The results of experiments are tested on the Middlebury evaluation system and outperform all the other PatchMatch based methods.
		</div></p>
	    <!-- ================================================================================================================================== -->
		
		<!-- ================================================================================================================================== -->
		<b><li>
		<!-- add title below -->
		Dense Correspondence Using Non-local DAISY Forest.
		</li></b>
		<!-- add authors below -->
		 Xiaoshui Huang, Jian Zhang, Qiang Wu, Chun Yuan, and Lixin Fan.
		<br><i> 
		<!-- add publication name below -->
		  International Conference on Digital Image Computing: Techniques and Applications (DICTA)
		</i>,
		<!-- add published year below -->
		2015
		.<br> 
		<!-- paper related links -->
		<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7371251" target="_blank">PDF</a><br>
		<!-- one figure to summarize the published work -->
		<img src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.com/blob/master/research/2015-daisy.PNG?raw=true" height="200" width="400"> <br> 
		<p><button class="accordion"> Abstract </button></p> <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
		<!-- add abstract below -->
		Dense correspondence computation is a critical computer vision task with many applications. The most existing dense correspondence methods consider all the neighbors connected to the center pixels and use local support region. However, such approach might only achieve a locally-optimal solution.In this paper, we propose a non-local dense correspondence computation method by calculating the match cost on a tree structure. It is non-local because all other nodes on the tree contribute to the match cost computing for the current node. The proposed method consists of three steps, namely: 1) DAISY descriptor computation, 2) edge-preserving segmentation and forest construction, 3) PatchMatch fast search. We test our algorithm on the Middlebury and Moseg datasets. The results show that the proposed method outperforms the state-of-the-art methods in dense correspondence computing and has a low computation complexity.
		</div></p>
	    <!-- ================================================================================================================================== -->
	</section>
	




    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
